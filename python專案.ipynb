{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f891769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#中時新聞網\n",
    "#導入套件\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time as t\n",
    "import datetime\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 輸入要搜尋的五檔股票\n",
    "search_lis =[]\n",
    "x1 ='長榮'\n",
    "search_lis.append(x1)\n",
    "x2 ='陽明'\n",
    "search_lis.append(x2)\n",
    "x3 ='萬海'\n",
    "search_lis.append(x3)\n",
    "x4 ='群創' \n",
    "search_lis.append(x4)\n",
    "x5 ='友達' \n",
    "search_lis.append(x5)\n",
    "\n",
    "# 先定義能判別日期的函數(只抓取近兩天的新聞)\n",
    "def distinguish_date(time):    \n",
    "    time = time.replace('/','-')\n",
    "    yest = datetime.datetime.now()+ datetime.timedelta(days=-1)\n",
    "    yest  = yest .strftime(\"%Y-%m-%d\")\n",
    "    if ( time == datetime.datetime.now().strftime(\"%Y-%m-%d\") ) or ( time == yest  ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 開始爬蟲的時間，為了區別每次爬蟲\n",
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# 每檔股票都跑一次\n",
    "for stock in search_lis:\n",
    "    title_lis = []\n",
    "    time_lis = []\n",
    "    source_lis = []\n",
    "    author_lis = []\n",
    "    article_url_lis = []\n",
    "    \n",
    "    # 跑每頁的內容\n",
    "    page = 1\n",
    "    while 1:    \n",
    "        url = \"https://wantrich.chinatimes.com/\"\n",
    "        url = url + 'search/' + stock + '/' + str(page)\n",
    "        re = requests.get(url)\n",
    "        soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "        news = soup.find('ul',{'class':'vertical-list'})\n",
    "        news = news.findAll('div', {'class':'articlebox-compact'})\n",
    "\n",
    "        for new in news:  \n",
    "            new_time = new.find('div', {'class':'meta-info'})\n",
    "            time = new_time.find('span', {'class':'date'}).text\n",
    "\n",
    "            if distinguish_date(time) == True :\n",
    "                new_title = new.find(\"h3\",{'class':'title'})\n",
    "                title = new_title.text       \n",
    "\n",
    "                article_url = new.find('a')['href']\n",
    "                article_url = urljoin(url,article_url )\n",
    "                \n",
    "                #隔頁爬蟲\n",
    "                re = requests.get(article_url)\n",
    "                soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "                soup = soup.find('div', {'class':'align-items-center'})\n",
    "                source = soup.find('div',{'class':'source'}).text\n",
    "                author = soup.find('div',{'class':'author'}).text\n",
    "                if author == '':\n",
    "                    author = '(無特定作者)'\n",
    "                # 確切新聞發布時間\n",
    "                time = time + '   ' + soup.find('span',{'class':'hour'}).text  \n",
    "\n",
    "                #\n",
    "                title_lis.append(title)\n",
    "                time_lis.append(time)\n",
    "                source_lis.append(source)\n",
    "                author_lis.append(author)\n",
    "                article_url_lis.append(article_url)\n",
    "                t.sleep(1)\n",
    "            elif distinguish_date(time) == False :\n",
    "                break\n",
    "    \n",
    "        if distinguish_date(time) == False :\n",
    "            break\n",
    "        page = page + 1\n",
    "        \n",
    "    df = pd.DataFrame({\"title\":title_lis,\"time\": time_lis ,\"author\":author_lis ,\"link\":article_url_lis})\n",
    "    # 設定檔案儲存路徑，並輸出成excel檔\n",
    "    path = \"D:\\python documents\"\n",
    "    path = os.path.join(path,'中時日報',start_time,stock)    \n",
    "    #print(stock)\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    string = stock + '_News_chinatimes.xlsx'\n",
    "    df.to_excel(os.path.join(path,string))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55555909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#經濟日報\n",
    "#導入套件\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time as t\n",
    "import datetime\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 輸入要搜尋的五檔股票\n",
    "search_lis =[]\n",
    "x1 ='長榮'\n",
    "search_lis.append(x1)\n",
    "x2 ='陽明'\n",
    "search_lis.append(x2)\n",
    "x3 ='萬海'\n",
    "search_lis.append(x3)\n",
    "x4 ='群創' \n",
    "search_lis.append(x4)\n",
    "x5 ='友達' \n",
    "search_lis.append(x5)\n",
    "\n",
    "# 先定義能判別日期的函數(只抓取近兩天的新聞)\n",
    "def distinguish_date(time):    \n",
    "    time = time.replace('/','-')\n",
    "    yest = datetime.datetime.now()+ datetime.timedelta(days=-1)\n",
    "    yest  = yest .strftime(\"%Y-%m-%d\")\n",
    "    if ( time == datetime.datetime.now().strftime(\"%Y-%m-%d\") ) or ( time == yest  ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 開始爬蟲的時間，為了區別每次爬蟲\n",
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# 每檔股票都跑一次\n",
    "for stock in search_lis:\n",
    "    title_lis = []\n",
    "    time_lis = []\n",
    "    author_lis = []\n",
    "    article_url_lis = []\n",
    "    \n",
    "    # 開始爬蟲\n",
    "    url = \"https://money.udn.com/search/result/1001/\"\n",
    "    url = url + stock\n",
    "    re = requests.get(url)\n",
    "    soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "    news = soup.find('ul',{'class':'story-list-holder'})\n",
    "    news = news.findAll('div', {'class':'story__content'})\n",
    "\n",
    "    for new in news:  \n",
    "        article_url = new.find('a')['href']\n",
    "        # 隔頁爬蟲\n",
    "        re = requests.get(article_url)\n",
    "        soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "        time = soup.find('time',{'class':'article-body__time'}).text \n",
    "        # time 格式  '%Y/%m/%d %H:%M:%S' \n",
    "        time_spli = time.split(\" \")\n",
    "        if distinguish_date(time_spli[0]) == True :\n",
    "            title = soup.find('h1',{'id':'story_art_title'}).text\n",
    "            article__url = article_url\n",
    "            \n",
    "            # refine author 資訊 \n",
    "            author = soup.find('div',{'class':'article-body__info'}).find('span').text\n",
    "            author_spli = author.split(\"／\")\n",
    "            author_spli =  author_spli[0]\n",
    "            author_spli = author_spli.split(\"/\")\n",
    "            author_spli =  author_spli[0].split(\" \")\n",
    "            #source = author_spli[0]\n",
    "            author = author_spli[1]\n",
    "            author = author.replace(\"記者\",'')\n",
    "            author = author.split(\"台北\")\n",
    "            author = author[0]\n",
    "            if author == '':\n",
    "                author = '(無特定作者)'\n",
    "            \n",
    "            #\n",
    "            title_lis.append(title)\n",
    "            time_lis.append(time)\n",
    "            author_lis.append(author)\n",
    "            article_url_lis.append(article__url)\n",
    "            t.sleep(1)\n",
    "        elif distinguish_date(time_spli[0]) == False :\n",
    "            break\n",
    "        \n",
    "    df = pd.DataFrame({\"title\":title_lis,\"time\": time_lis ,\"author\":author_lis ,\"link\":article_url_lis})\n",
    "    # 設定檔案儲存路徑，並輸出成excel檔\n",
    "    path = \"D:\\python documents\"\n",
    "    path = os.path.join(path,'經濟日報',start_time,stock) \n",
    "    #print(stock)\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    string = stock + '_News_economy.xlsx'\n",
    "    df.to_excel(os.path.join(path,string))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345bc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
