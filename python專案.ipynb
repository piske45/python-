{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f891769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#中時新聞網\n",
    "#導入套件\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time as t\n",
    "import datetime\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 輸入要搜尋的五檔股票\n",
    "search_lis =[]\n",
    "x1 ='長榮'\n",
    "search_lis.append(x1)\n",
    "x2 ='陽明'\n",
    "search_lis.append(x2)\n",
    "x3 ='萬海'\n",
    "search_lis.append(x3)\n",
    "x4 ='群創' \n",
    "search_lis.append(x4)\n",
    "x5 ='友達' \n",
    "search_lis.append(x5)\n",
    "\n",
    "# 先定義能判別日期的函數(只抓取近兩天的新聞)\n",
    "def distinguish_date(time):    \n",
    "    time = time.replace('/','-')\n",
    "    yest = datetime.datetime.now()+ datetime.timedelta(days=-1)\n",
    "    yest  = yest .strftime(\"%Y-%m-%d\")\n",
    "    if ( time == datetime.datetime.now().strftime(\"%Y-%m-%d\") ) or ( time == yest  ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 開始爬蟲的時間，為了區別每次爬蟲\n",
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# 每檔股票都跑一次\n",
    "for stock in search_lis:\n",
    "    title_lis = []\n",
    "    time_lis = []\n",
    "    source_lis = []\n",
    "    author_lis = []\n",
    "    article_url_lis = []\n",
    "    \n",
    "    # 跑每頁的內容\n",
    "    page = 1\n",
    "    while 1:    \n",
    "        url = \"https://wantrich.chinatimes.com/\"\n",
    "        url = url + 'search/' + stock + '/' + str(page)\n",
    "        re = requests.get(url)\n",
    "        soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "        news = soup.find('ul',{'class':'vertical-list'})\n",
    "        news = news.findAll('div', {'class':'articlebox-compact'})\n",
    "\n",
    "        for new in news:  \n",
    "            new_time = new.find('div', {'class':'meta-info'})\n",
    "            time = new_time.find('span', {'class':'date'}).text\n",
    "\n",
    "            if distinguish_date(time) == True :\n",
    "                new_title = new.find(\"h3\",{'class':'title'})\n",
    "                title = new_title.text       \n",
    "\n",
    "                article_url = new.find('a')['href']\n",
    "                article_url = urljoin(url,article_url )\n",
    "                \n",
    "                #隔頁爬蟲\n",
    "                re = requests.get(article_url)\n",
    "                soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "                soup = soup.find('div', {'class':'align-items-center'})\n",
    "                source = soup.find('div',{'class':'source'}).text\n",
    "                author = soup.find('div',{'class':'author'}).text\n",
    "                if author == '':\n",
    "                    author = '(無特定作者)'\n",
    "                # 確切新聞發布時間\n",
    "                time = time + '   ' + soup.find('span',{'class':'hour'}).text  \n",
    "\n",
    "                #\n",
    "                title_lis.append(title)\n",
    "                time_lis.append(time)\n",
    "                source_lis.append(source)\n",
    "                author_lis.append(author)\n",
    "                article_url_lis.append(article_url)\n",
    "                t.sleep(1)\n",
    "            elif distinguish_date(time) == False :\n",
    "                break\n",
    "    \n",
    "        if distinguish_date(time) == False :\n",
    "            break\n",
    "        page = page + 1\n",
    "        \n",
    "    df = pd.DataFrame({\"title\":title_lis,\"time\": time_lis ,\"author\":author_lis ,\"link\":article_url_lis})\n",
    "    # 設定檔案儲存路徑，並輸出成excel檔\n",
    "    path = \"D:\\python documents\"\n",
    "    path = os.path.join(path,'中時日報',start_time,stock)    \n",
    "    #print(stock)\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    string = stock + '_News_chinatimes.xlsx'\n",
    "    df.to_excel(os.path.join(path,string))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55555909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#經濟日報\n",
    "#導入套件\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time as t\n",
    "import datetime\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 輸入要搜尋的五檔股票\n",
    "search_lis =[]\n",
    "x1 ='長榮'\n",
    "search_lis.append(x1)\n",
    "x2 ='陽明'\n",
    "search_lis.append(x2)\n",
    "x3 ='萬海'\n",
    "search_lis.append(x3)\n",
    "x4 ='群創' \n",
    "search_lis.append(x4)\n",
    "x5 ='友達' \n",
    "search_lis.append(x5)\n",
    "\n",
    "# 先定義能判別日期的函數(只抓取近兩天的新聞)\n",
    "def distinguish_date(time):    \n",
    "    time = time.replace('/','-')\n",
    "    yest = datetime.datetime.now()+ datetime.timedelta(days=-1)\n",
    "    yest  = yest .strftime(\"%Y-%m-%d\")\n",
    "    if ( time == datetime.datetime.now().strftime(\"%Y-%m-%d\") ) or ( time == yest  ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 開始爬蟲的時間，為了區別每次爬蟲\n",
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# 每檔股票都跑一次\n",
    "for stock in search_lis:\n",
    "    title_lis = []\n",
    "    time_lis = []\n",
    "    author_lis = []\n",
    "    article_url_lis = []\n",
    "    \n",
    "    # 開始爬蟲\n",
    "    url = \"https://money.udn.com/search/result/1001/\"\n",
    "    url = url + stock\n",
    "    re = requests.get(url)\n",
    "    soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "    news = soup.find('ul',{'class':'story-list-holder'})\n",
    "    news = news.findAll('div', {'class':'story__content'})\n",
    "\n",
    "    for new in news:  \n",
    "        article_url = new.find('a')['href']\n",
    "        # 隔頁爬蟲\n",
    "        re = requests.get(article_url)\n",
    "        soup = BeautifulSoup(re.text, \"html.parser\")\n",
    "        time = soup.find('time',{'class':'article-body__time'}).text \n",
    "        # time 格式  '%Y/%m/%d %H:%M:%S' \n",
    "        time_spli = time.split(\" \")\n",
    "        if distinguish_date(time_spli[0]) == True :\n",
    "            title = soup.find('h1',{'id':'story_art_title'}).text\n",
    "            article__url = article_url\n",
    "            \n",
    "            # refine author 資訊 \n",
    "            author = soup.find('div',{'class':'article-body__info'}).find('span').text\n",
    "            author_spli = author.split(\"／\")\n",
    "            author_spli =  author_spli[0]\n",
    "            author_spli = author_spli.split(\"/\")\n",
    "            author_spli =  author_spli[0].split(\" \")\n",
    "            #source = author_spli[0]\n",
    "            author = author_spli[1]\n",
    "            author = author.replace(\"記者\",'')\n",
    "            author = author.split(\"台北\")\n",
    "            author = author[0]\n",
    "            if author == '':\n",
    "                author = '(無特定作者)'\n",
    "            \n",
    "            #\n",
    "            title_lis.append(title)\n",
    "            time_lis.append(time)\n",
    "            author_lis.append(author)\n",
    "            article_url_lis.append(article__url)\n",
    "            t.sleep(1)\n",
    "        elif distinguish_date(time_spli[0]) == False :\n",
    "            break\n",
    "        \n",
    "    df = pd.DataFrame({\"title\":title_lis,\"time\": time_lis ,\"author\":author_lis ,\"link\":article_url_lis})\n",
    "    # 設定檔案儲存路徑，並輸出成excel檔\n",
    "    path = \"D:\\python documents\"\n",
    "    path = os.path.join(path,'經濟日報',start_time,stock) \n",
    "    #print(stock)\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    string = stock + '_News_economy.xlsx'\n",
    "    df.to_excel(os.path.join(path,string))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f345bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5a01c47cf96e>:49: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"C:/ProgramData/Anaconda3/Lib/site-packages/selenium/chromedriver.exe\",options =my_options)\n",
      "<ipython-input-6-5a01c47cf96e>:53: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  news = driver.find_element_by_xpath('//*[@id=\"_SearchAll\"]/section/div').find_elements_by_tag_name('a'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.cnyes.com/news/id/4789006\n",
      "〈航運指數〉SCFI續創高 美西、美東線運價連袂漲上新高\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5a01c47cf96e>:75: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  author = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/main/div[2]/div[2]/span/span').text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鉅亨網記者王莞甯 台北\n",
      "王莞甯\n",
      "2021/12/17 17:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5a01c47cf96e>:86: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  time = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/main/div[2]/div[2]/time').text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "長榮\n",
      "https://news.cnyes.com/news/id/4789006\n",
      "〈航運指數〉SCFI續創高 美西、美東線運價連袂漲上新高\n",
      "鉅亨網記者王莞甯 台北\n",
      "王莞甯\n",
      "2021/12/17 17:41\n",
      "陽明\n",
      "https://news.cnyes.com/news/id/4789006\n",
      "〈航運指數〉SCFI續創高 美西、美東線運價連袂漲上新高\n",
      "鉅亨網記者王莞甯 台北\n",
      "王莞甯\n",
      "2021/12/17 17:41\n",
      "萬海\n",
      "https://news.cnyes.com/news/id/4789023\n",
      "三大法人買超台股57億元 內外資同步敲進中鋼近10萬張\n",
      "鉅亨網記者郭幸宜 台北\n",
      "郭幸宜\n",
      "2021/12/17 17:54\n",
      "群創\n",
      "https://news.cnyes.com/news/id/4789305\n",
      "Fed決策轉鷹、英特爾CEO抵台、大同高層再異動 本周大事回顧\n",
      "鉅亨網記者魏志豪 台北\n",
      "魏志豪\n",
      "2021/12/18 15:04\n",
      "https://news.cnyes.com/news/id/4789023\n",
      "三大法人買超台股57億元 內外資同步敲進中鋼近10萬張\n",
      "鉅亨網記者郭幸宜 台北\n",
      "郭幸宜\n",
      "2021/12/17 17:54\n",
      "友達\n"
     ]
    }
   ],
   "source": [
    "# 鉅亨網\n",
    "# 導入套件\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import datetime\n",
    "import os\n",
    "# 輸入要搜尋的五檔股票\n",
    "search_lis =[]\n",
    "x1 ='長榮'\n",
    "search_lis.append(x1)\n",
    "x2 ='陽明'\n",
    "search_lis.append(x2)\n",
    "x3 ='萬海'\n",
    "search_lis.append(x3)\n",
    "x4 ='群創' \n",
    "search_lis.append(x4)\n",
    "x5 ='友達' \n",
    "search_lis.append(x5)\n",
    "\n",
    "# 先定義能判別日期的函數(只抓取近兩天的新聞)\n",
    "def distinguish_date(time):    \n",
    "    time = time.replace('/','-')\n",
    "    time_spl = time.split(' ')\n",
    "    time =  time_spl[0]\n",
    "    #yest = datetime.datetime.now()+ datetime.timedelta(days=-1)\n",
    "    #yest  = yest .strftime(\"%Y-%m-%d\")\n",
    "    if ( time == datetime.datetime.now().strftime(\"%Y-%m-%d\") ):     #or ( time == yest  ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# 開始爬蟲的時間，為了區別每次爬蟲\n",
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "# 開始抓取新聞\n",
    "# 每檔股票都跑一次\n",
    "for stock in search_lis:\n",
    "    title_lis = []\n",
    "    time_lis = []\n",
    "    author_lis = []\n",
    "    article_url_lis = []\n",
    "    \n",
    "    my_options = Options()\n",
    "    # my_options.add_argument(“--headless”)\n",
    "    my_options.add_argument(\"--incognito\")\n",
    "    driver = webdriver.Chrome(\"C:/ProgramData/Anaconda3/Lib/site-packages/selenium/chromedriver.exe\",options =my_options)\n",
    "    #driver = webdriver.Chrome()\n",
    "    url = 'https://www.cnyes.com/search/news?keyword=' + stock\n",
    "    driver.get(url)   \n",
    "    news = driver.find_element_by_xpath('//*[@id=\"_SearchAll\"]/section/div').find_elements_by_tag_name('a'\n",
    "                                                                                                      )\n",
    "    for i in range(len(news)):\n",
    "        article_url = news[i].get_attribute('href')   \n",
    "        print(article_url)\n",
    "\n",
    "        \n",
    "        #news_head = driver.find_element_by_xpath('//*[@id=\"_SearchAll\"]/section/div')\n",
    "        news_head = news[i].find_element_by_tag_name('h3')\n",
    "        title = news_head.text\n",
    "        print(title)        \n",
    "        #獲取當前視窗控制程式碼\n",
    "        now_handle = driver.current_window_handle \n",
    "        # 點擊文章網址\n",
    "        news_head.click()                \n",
    "        #獲取所有視窗控制程式碼\n",
    "        all_handles = driver.window_handles\n",
    "        #彈出兩個介面,跳轉到不是主窗體介面\n",
    "        for handle in all_handles:\n",
    "            if handle != now_handle:\n",
    "                driver.switch_to.window(handle)\n",
    "                try:\n",
    "                    author = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/main/div[2]/div[2]/span/span').text\n",
    "                except NoSuchElementException or StaleElementReferenceException or ConnectionRefusedError:\n",
    "                    author = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/main/div[2]/div/span/span').text\n",
    "\n",
    "                #author = author.text\n",
    "                print(author)\n",
    "                author_spli = author.split(' ')\n",
    "                if author_spli[0] == author:\n",
    "                    author = '(無特定作者)'\n",
    "                author = author_spli[0].replace('鉅亨網記者','')\n",
    "                print(author)\n",
    "                time = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[2]/main/div[2]/div[2]/time').text      \n",
    "                print(time)\n",
    "                if distinguish_date(time) == True: \n",
    "                    author_lis.append(author)\n",
    "                    title_lis.append(title)\n",
    "                    article_url_lis.append( article_url)\n",
    "                    time_lis.append(time)\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(now_handle)\n",
    "                    t.sleep(1)\n",
    "                elif distinguish_date(time) == False:\n",
    "                    driver.quit()\n",
    "                    break\n",
    "        if distinguish_date(time) == False:\n",
    "            break\n",
    "    df = pd.DataFrame({\"title\":title_lis,\"time\": time_lis ,\"author\":author_lis ,\"link\":article_url_lis})\n",
    "    # 設定檔案儲存路徑，並輸出成excel檔\n",
    "    path = \"D:\\python documents\"\n",
    "    path = os.path.join(path,'鉅亨網',start_time,stock) \n",
    "    print(stock)\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    string = stock + '_News_cnye.xlsx'\n",
    "    df.to_excel(os.path.join(path,string))    \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
